{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import cast, expand_dims,constant,float32\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock Name_AAPL</th>\n",
       "      <th>Stock Name_AMD</th>\n",
       "      <th>Stock Name_AMZN</th>\n",
       "      <th>Stock Name_BA</th>\n",
       "      <th>Stock Name_BX</th>\n",
       "      <th>Stock Name_COST</th>\n",
       "      <th>Stock Name_CRM</th>\n",
       "      <th>Stock Name_DIS</th>\n",
       "      <th>Stock Name_ENPH</th>\n",
       "      <th>Stock Name_F</th>\n",
       "      <th>...</th>\n",
       "      <th>scaler__scaler__Sentiment Score</th>\n",
       "      <th>scaler__scaler__Tweet Count</th>\n",
       "      <th>scaler__scaler__Open</th>\n",
       "      <th>scaler__scaler__High</th>\n",
       "      <th>scaler__scaler__Low</th>\n",
       "      <th>scaler__scaler__Adj Close</th>\n",
       "      <th>scaler__scaler__Volume</th>\n",
       "      <th>scaler__scaler__Open/Close Diff</th>\n",
       "      <th>scaler__scaler__Prev Close Diff</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538566</td>\n",
       "      <td>0.240781</td>\n",
       "      <td>0.398767</td>\n",
       "      <td>0.394835</td>\n",
       "      <td>0.377466</td>\n",
       "      <td>0.377952</td>\n",
       "      <td>0.283144</td>\n",
       "      <td>0.275307</td>\n",
       "      <td>0.373179</td>\n",
       "      <td>268.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499647</td>\n",
       "      <td>0.240781</td>\n",
       "      <td>0.398767</td>\n",
       "      <td>0.394835</td>\n",
       "      <td>0.377466</td>\n",
       "      <td>0.377952</td>\n",
       "      <td>0.283144</td>\n",
       "      <td>0.275307</td>\n",
       "      <td>0.373179</td>\n",
       "      <td>268.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648871</td>\n",
       "      <td>0.240781</td>\n",
       "      <td>0.398767</td>\n",
       "      <td>0.394835</td>\n",
       "      <td>0.377466</td>\n",
       "      <td>0.377952</td>\n",
       "      <td>0.283144</td>\n",
       "      <td>0.275307</td>\n",
       "      <td>0.373179</td>\n",
       "      <td>268.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118119</td>\n",
       "      <td>0.240781</td>\n",
       "      <td>0.398767</td>\n",
       "      <td>0.394835</td>\n",
       "      <td>0.377466</td>\n",
       "      <td>0.377952</td>\n",
       "      <td>0.283144</td>\n",
       "      <td>0.275307</td>\n",
       "      <td>0.373179</td>\n",
       "      <td>268.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058530</td>\n",
       "      <td>0.240781</td>\n",
       "      <td>0.398767</td>\n",
       "      <td>0.394835</td>\n",
       "      <td>0.377466</td>\n",
       "      <td>0.377952</td>\n",
       "      <td>0.283144</td>\n",
       "      <td>0.275307</td>\n",
       "      <td>0.373179</td>\n",
       "      <td>268.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63671</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037872</td>\n",
       "      <td>0.039905</td>\n",
       "      <td>0.038376</td>\n",
       "      <td>0.040180</td>\n",
       "      <td>0.044713</td>\n",
       "      <td>0.025733</td>\n",
       "      <td>0.401610</td>\n",
       "      <td>38.259998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63672</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035347</td>\n",
       "      <td>0.034569</td>\n",
       "      <td>0.033459</td>\n",
       "      <td>0.034084</td>\n",
       "      <td>0.021957</td>\n",
       "      <td>0.020246</td>\n",
       "      <td>0.396646</td>\n",
       "      <td>34.110001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63673</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712140</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.037651</td>\n",
       "      <td>0.036831</td>\n",
       "      <td>0.035769</td>\n",
       "      <td>0.036067</td>\n",
       "      <td>0.026417</td>\n",
       "      <td>0.024409</td>\n",
       "      <td>0.398279</td>\n",
       "      <td>35.459999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63674</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773039</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.037651</td>\n",
       "      <td>0.036831</td>\n",
       "      <td>0.035769</td>\n",
       "      <td>0.036067</td>\n",
       "      <td>0.026417</td>\n",
       "      <td>0.024409</td>\n",
       "      <td>0.398279</td>\n",
       "      <td>35.459999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63675</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035127</td>\n",
       "      <td>0.035889</td>\n",
       "      <td>0.035378</td>\n",
       "      <td>0.036185</td>\n",
       "      <td>0.021605</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.230766</td>\n",
       "      <td>35.540001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63676 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Stock Name_AAPL  Stock Name_AMD  Stock Name_AMZN  Stock Name_BA  \\\n",
       "0                    0               0                0              0   \n",
       "1                    0               0                0              0   \n",
       "2                    0               0                0              0   \n",
       "3                    0               0                0              0   \n",
       "4                    0               0                0              0   \n",
       "...                ...             ...              ...            ...   \n",
       "63671                0               0                0              0   \n",
       "63672                0               0                0              0   \n",
       "63673                0               0                0              0   \n",
       "63674                0               0                0              0   \n",
       "63675                0               0                0              0   \n",
       "\n",
       "       Stock Name_BX  Stock Name_COST  Stock Name_CRM  Stock Name_DIS  \\\n",
       "0                  0                0               0               0   \n",
       "1                  0                0               0               0   \n",
       "2                  0                0               0               0   \n",
       "3                  0                0               0               0   \n",
       "4                  0                0               0               0   \n",
       "...              ...              ...             ...             ...   \n",
       "63671              0                0               0               0   \n",
       "63672              0                0               0               0   \n",
       "63673              0                0               0               0   \n",
       "63674              0                0               0               0   \n",
       "63675              0                0               0               0   \n",
       "\n",
       "       Stock Name_ENPH  Stock Name_F  ...  scaler__scaler__Sentiment Score  \\\n",
       "0                    0             0  ...                         0.538566   \n",
       "1                    0             0  ...                         0.499647   \n",
       "2                    0             0  ...                         0.648871   \n",
       "3                    0             0  ...                         0.118119   \n",
       "4                    0             0  ...                         0.058530   \n",
       "...                ...           ...  ...                              ...   \n",
       "63671                0             0  ...                         0.589333   \n",
       "63672                0             0  ...                         0.692125   \n",
       "63673                0             0  ...                         0.712140   \n",
       "63674                0             0  ...                         0.773039   \n",
       "63675                0             0  ...                         0.892015   \n",
       "\n",
       "       scaler__scaler__Tweet Count  scaler__scaler__Open  \\\n",
       "0                         0.240781              0.398767   \n",
       "1                         0.240781              0.398767   \n",
       "2                         0.240781              0.398767   \n",
       "3                         0.240781              0.398767   \n",
       "4                         0.240781              0.398767   \n",
       "...                            ...                   ...   \n",
       "63671                     0.000000              0.037872   \n",
       "63672                     0.000000              0.035347   \n",
       "63673                     0.002169              0.037651   \n",
       "63674                     0.002169              0.037651   \n",
       "63675                     0.000000              0.035127   \n",
       "\n",
       "       scaler__scaler__High  scaler__scaler__Low  scaler__scaler__Adj Close  \\\n",
       "0                  0.394835             0.377466                   0.377952   \n",
       "1                  0.394835             0.377466                   0.377952   \n",
       "2                  0.394835             0.377466                   0.377952   \n",
       "3                  0.394835             0.377466                   0.377952   \n",
       "4                  0.394835             0.377466                   0.377952   \n",
       "...                     ...                  ...                        ...   \n",
       "63671              0.039905             0.038376                   0.040180   \n",
       "63672              0.034569             0.033459                   0.034084   \n",
       "63673              0.036831             0.035769                   0.036067   \n",
       "63674              0.036831             0.035769                   0.036067   \n",
       "63675              0.035889             0.035378                   0.036185   \n",
       "\n",
       "       scaler__scaler__Volume  scaler__scaler__Open/Close Diff  \\\n",
       "0                    0.283144                         0.275307   \n",
       "1                    0.283144                         0.275307   \n",
       "2                    0.283144                         0.275307   \n",
       "3                    0.283144                         0.275307   \n",
       "4                    0.283144                         0.275307   \n",
       "...                       ...                              ...   \n",
       "63671                0.044713                         0.025733   \n",
       "63672                0.021957                         0.020246   \n",
       "63673                0.026417                         0.024409   \n",
       "63674                0.026417                         0.024409   \n",
       "63675                0.021605                         0.009650   \n",
       "\n",
       "       scaler__scaler__Prev Close Diff       Close  \n",
       "0                             0.373179  268.209991  \n",
       "1                             0.373179  268.209991  \n",
       "2                             0.373179  268.209991  \n",
       "3                             0.373179  268.209991  \n",
       "4                             0.373179  268.209991  \n",
       "...                                ...         ...  \n",
       "63671                         0.401610   38.259998  \n",
       "63672                         0.396646   34.110001  \n",
       "63673                         0.398279   35.459999  \n",
       "63674                         0.398279   35.459999  \n",
       "63675                         0.230766   35.540001  \n",
       "\n",
       "[63676 rows x 60 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df = pd.read_csv('Resources/final_stock_data.csv',header='infer')\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stock_df.drop(columns=['Close'])\n",
    "y = stock_df['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.5]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = cast(constant([0.5]),dtype=float32)\n",
    "y = cast(constant([0.6]),dtype=float32)\n",
    "X = expand_dims(X, axis=0)\n",
    "y = expand_dims(y, axis=0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = expand_dims(X_train, axis=0)\n",
    "y_train = expand_dims(y_train, axis=0)\n",
    "\n",
    "X_test = expand_dims(X_test, axis=0)\n",
    "y_test = expand_dims(y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50940, 59)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kille\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_nodes = len(X_train.columns)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units=18,activation='relu',input_dim=input_nodes))\n",
    "#model.add(keras.layers.AveragePooling1D(pool_size=2,strides=2))\n",
    "model.add(keras.layers.Dense(units=8, activation=\"relu\"))\n",
    "#model.add(keras.layers.AveragePooling1D(pool_size=2,strides=2))\n",
    "model.add(keras.layers.Dense(units=4,activation='relu'))\n",
    "model.add(keras.layers.Dense(units=8, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=4,activation='relu'))\n",
    "model.add(keras.layers.Dense(units=8,activation='relu'))\n",
    "model.add(keras.layers.Dense(1,activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_percentage_error', metrics=[keras.metrics.MeanAbsolutePercentageError(),'accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 99.2176 - mean_absolute_error: 227.7130\n",
      "Epoch 2/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 92.5977 - mean_absolute_error: 218.4499\n",
      "Epoch 3/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 83.6298 - mean_absolute_error: 202.4332\n",
      "Epoch 4/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 76.7983 - mean_absolute_error: 185.6982\n",
      "Epoch 5/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 69.5078 - mean_absolute_error: 165.8398\n",
      "Epoch 6/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 62.9935 - mean_absolute_error: 144.4247\n",
      "Epoch 7/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 60.1003 - mean_absolute_error: 128.5961\n",
      "Epoch 8/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.1216 - mean_absolute_error: 122.3996\n",
      "Epoch 9/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 58.9693 - mean_absolute_error: 121.1991\n",
      "Epoch 10/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 58.5127 - mean_absolute_error: 120.7946\n",
      "Epoch 11/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.4100 - mean_absolute_error: 120.3284\n",
      "Epoch 12/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.3262 - mean_absolute_error: 121.3524\n",
      "Epoch 13/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 58.5537 - mean_absolute_error: 120.7287\n",
      "Epoch 14/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 58.9746 - mean_absolute_error: 120.6077\n",
      "Epoch 15/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.4710 - mean_absolute_error: 121.2869\n",
      "Epoch 16/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.1538 - mean_absolute_error: 120.3923\n",
      "Epoch 17/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.3591 - mean_absolute_error: 120.5817\n",
      "Epoch 18/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.0119 - mean_absolute_error: 120.6080\n",
      "Epoch 19/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.7410 - mean_absolute_error: 120.8010\n",
      "Epoch 20/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.6624 - mean_absolute_error: 120.5808\n",
      "Epoch 21/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 58.9416 - mean_absolute_error: 121.0954\n",
      "Epoch 22/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.2115 - mean_absolute_error: 120.4049\n",
      "Epoch 23/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.0334 - mean_absolute_error: 120.4609\n",
      "Epoch 24/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.2238 - mean_absolute_error: 121.2669\n",
      "Epoch 25/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.1888 - mean_absolute_error: 120.7022\n",
      "Epoch 26/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.7118 - mean_absolute_error: 120.6568\n",
      "Epoch 27/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 58.7843 - mean_absolute_error: 120.4911\n",
      "Epoch 28/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 58.9555 - mean_absolute_error: 121.0962\n",
      "Epoch 29/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.4634 - mean_absolute_error: 121.5526\n",
      "Epoch 30/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.3036 - mean_absolute_error: 120.9742\n",
      "Epoch 31/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 58.7145 - mean_absolute_error: 120.1081\n",
      "Epoch 32/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 58.8760 - mean_absolute_error: 120.6383\n",
      "Epoch 33/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 58.5661 - mean_absolute_error: 120.7135\n",
      "Epoch 34/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 58.5794 - mean_absolute_error: 120.7288\n",
      "Epoch 35/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.1185 - mean_absolute_error: 121.1393\n",
      "Epoch 36/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 58.7760 - mean_absolute_error: 120.9859\n",
      "Epoch 37/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.2383 - mean_absolute_error: 121.6924\n",
      "Epoch 38/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.1614 - mean_absolute_error: 121.2130\n",
      "Epoch 39/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 58.9719 - mean_absolute_error: 120.6339\n",
      "Epoch 40/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.5507 - mean_absolute_error: 120.6717\n",
      "Epoch 41/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 58.6569 - mean_absolute_error: 120.2725\n",
      "Epoch 42/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.0815 - mean_absolute_error: 120.9278\n",
      "Epoch 43/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.2145 - mean_absolute_error: 120.2334\n",
      "Epoch 44/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.0736 - mean_absolute_error: 121.5270\n",
      "Epoch 45/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.6096 - mean_absolute_error: 121.6330\n",
      "Epoch 46/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 58.7349 - mean_absolute_error: 120.7049\n",
      "Epoch 47/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 59.2014 - mean_absolute_error: 120.5692\n",
      "Epoch 48/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.0388 - mean_absolute_error: 119.9936\n",
      "Epoch 49/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1018 - mean_absolute_error: 120.0457\n",
      "Epoch 50/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.4247 - mean_absolute_error: 120.1698\n",
      "Epoch 51/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.9422 - mean_absolute_error: 121.3372\n",
      "Epoch 52/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7623 - mean_absolute_error: 121.3508\n",
      "Epoch 53/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2410 - mean_absolute_error: 120.4314\n",
      "Epoch 54/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9800 - mean_absolute_error: 120.9064\n",
      "Epoch 55/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.7516 - mean_absolute_error: 121.1390\n",
      "Epoch 56/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1955 - mean_absolute_error: 120.7471\n",
      "Epoch 57/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4911 - mean_absolute_error: 121.2985\n",
      "Epoch 58/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0999 - mean_absolute_error: 121.1963\n",
      "Epoch 59/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 60.0247 - mean_absolute_error: 121.6379\n",
      "Epoch 60/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1589 - mean_absolute_error: 120.5724\n",
      "Epoch 61/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.5967 - mean_absolute_error: 121.0482\n",
      "Epoch 62/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.5897 - mean_absolute_error: 120.5476\n",
      "Epoch 63/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3861 - mean_absolute_error: 121.7882\n",
      "Epoch 64/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7729 - mean_absolute_error: 120.4481\n",
      "Epoch 65/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2694 - mean_absolute_error: 121.0953\n",
      "Epoch 66/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.6727 - mean_absolute_error: 121.4682\n",
      "Epoch 67/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7511 - mean_absolute_error: 121.0754\n",
      "Epoch 68/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0659 - mean_absolute_error: 121.3499\n",
      "Epoch 69/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.1942 - mean_absolute_error: 120.3525\n",
      "Epoch 70/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2919 - mean_absolute_error: 120.7976\n",
      "Epoch 71/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.2343 - mean_absolute_error: 120.5004\n",
      "Epoch 72/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3689 - mean_absolute_error: 121.7954\n",
      "Epoch 73/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4230 - mean_absolute_error: 120.9943\n",
      "Epoch 74/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4422 - mean_absolute_error: 120.2291\n",
      "Epoch 75/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1595 - mean_absolute_error: 120.9185\n",
      "Epoch 76/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7109 - mean_absolute_error: 120.9918\n",
      "Epoch 77/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 59.5096 - mean_absolute_error: 121.2334\n",
      "Epoch 78/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0183 - mean_absolute_error: 120.8787\n",
      "Epoch 79/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7711 - mean_absolute_error: 120.5894\n",
      "Epoch 80/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.6150 - mean_absolute_error: 120.9555\n",
      "Epoch 81/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.5403 - mean_absolute_error: 120.2345\n",
      "Epoch 82/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0107 - mean_absolute_error: 120.8385\n",
      "Epoch 83/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.5925 - mean_absolute_error: 121.3498\n",
      "Epoch 84/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3926 - mean_absolute_error: 120.4885\n",
      "Epoch 85/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0355 - mean_absolute_error: 121.0769\n",
      "Epoch 86/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2203 - mean_absolute_error: 121.0900\n",
      "Epoch 87/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3089 - mean_absolute_error: 121.2007\n",
      "Epoch 88/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.5498 - mean_absolute_error: 119.8377\n",
      "Epoch 89/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1232 - mean_absolute_error: 120.8528\n",
      "Epoch 90/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8529 - mean_absolute_error: 120.1466\n",
      "Epoch 91/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0458 - mean_absolute_error: 121.2648\n",
      "Epoch 92/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2446 - mean_absolute_error: 120.6980\n",
      "Epoch 93/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9739 - mean_absolute_error: 121.0562\n",
      "Epoch 94/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.6848 - mean_absolute_error: 120.9784\n",
      "Epoch 95/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9152 - mean_absolute_error: 120.7076\n",
      "Epoch 96/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7188 - mean_absolute_error: 120.3589\n",
      "Epoch 97/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.6098 - mean_absolute_error: 119.8586\n",
      "Epoch 98/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9753 - mean_absolute_error: 121.0861\n",
      "Epoch 99/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1732 - mean_absolute_error: 121.7693\n",
      "Epoch 100/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2052 - mean_absolute_error: 121.2372\n",
      "Epoch 101/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.7093 - mean_absolute_error: 120.6389\n",
      "Epoch 102/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7064 - mean_absolute_error: 120.8544\n",
      "Epoch 103/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3150 - mean_absolute_error: 120.7892\n",
      "Epoch 104/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8423 - mean_absolute_error: 120.3028\n",
      "Epoch 105/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.7269 - mean_absolute_error: 121.3037\n",
      "Epoch 106/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.6169 - mean_absolute_error: 120.7647\n",
      "Epoch 107/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8697 - mean_absolute_error: 120.5651\n",
      "Epoch 108/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9798 - mean_absolute_error: 120.7991\n",
      "Epoch 109/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8857 - mean_absolute_error: 120.2560\n",
      "Epoch 110/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.2602 - mean_absolute_error: 120.1343\n",
      "Epoch 111/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2391 - mean_absolute_error: 120.3229\n",
      "Epoch 112/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1442 - mean_absolute_error: 120.8261\n",
      "Epoch 113/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0762 - mean_absolute_error: 120.7069\n",
      "Epoch 114/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1460 - mean_absolute_error: 121.2707\n",
      "Epoch 115/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1332 - mean_absolute_error: 121.1266\n",
      "Epoch 116/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.5835 - mean_absolute_error: 121.2654\n",
      "Epoch 117/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0932 - mean_absolute_error: 121.6659\n",
      "Epoch 118/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2611 - mean_absolute_error: 120.9467\n",
      "Epoch 119/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3882 - mean_absolute_error: 120.9770\n",
      "Epoch 120/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.6692 - mean_absolute_error: 121.2118\n",
      "Epoch 121/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3174 - mean_absolute_error: 121.1522\n",
      "Epoch 122/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2210 - mean_absolute_error: 121.1065\n",
      "Epoch 123/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2862 - mean_absolute_error: 121.0185\n",
      "Epoch 124/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0529 - mean_absolute_error: 120.5445\n",
      "Epoch 125/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0622 - mean_absolute_error: 121.1108\n",
      "Epoch 126/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7508 - mean_absolute_error: 120.1719\n",
      "Epoch 127/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2020 - mean_absolute_error: 120.6486\n",
      "Epoch 128/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.0472 - mean_absolute_error: 120.1587\n",
      "Epoch 129/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4689 - mean_absolute_error: 121.6174\n",
      "Epoch 130/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3381 - mean_absolute_error: 121.2851\n",
      "Epoch 131/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1020 - mean_absolute_error: 121.2850\n",
      "Epoch 132/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2950 - mean_absolute_error: 120.8776\n",
      "Epoch 133/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.7351 - mean_absolute_error: 121.3709\n",
      "Epoch 134/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.5895 - mean_absolute_error: 120.0921\n",
      "Epoch 135/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2912 - mean_absolute_error: 121.0843\n",
      "Epoch 136/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1167 - mean_absolute_error: 121.0223\n",
      "Epoch 137/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9625 - mean_absolute_error: 120.6358\n",
      "Epoch 138/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8240 - mean_absolute_error: 120.1675\n",
      "Epoch 139/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8913 - mean_absolute_error: 120.5861\n",
      "Epoch 140/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.6324 - mean_absolute_error: 121.1371\n",
      "Epoch 141/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4868 - mean_absolute_error: 121.1943\n",
      "Epoch 142/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9173 - mean_absolute_error: 120.1491\n",
      "Epoch 143/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8622 - mean_absolute_error: 120.3936\n",
      "Epoch 144/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3311 - mean_absolute_error: 120.9286\n",
      "Epoch 145/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9354 - mean_absolute_error: 120.9396\n",
      "Epoch 146/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2475 - mean_absolute_error: 120.8305\n",
      "Epoch 147/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2688 - mean_absolute_error: 120.9255\n",
      "Epoch 148/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.6086 - mean_absolute_error: 120.7915\n",
      "Epoch 149/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2014 - mean_absolute_error: 120.6846\n",
      "Epoch 150/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9991 - mean_absolute_error: 121.2722\n",
      "Epoch 151/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2738 - mean_absolute_error: 121.1245\n",
      "Epoch 152/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1233 - mean_absolute_error: 121.2863\n",
      "Epoch 153/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3181 - mean_absolute_error: 121.0092\n",
      "Epoch 154/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.8778 - mean_absolute_error: 121.4320\n",
      "Epoch 155/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.5994 - mean_absolute_error: 120.6290\n",
      "Epoch 156/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.5639 - mean_absolute_error: 121.2289\n",
      "Epoch 157/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9624 - mean_absolute_error: 120.5596\n",
      "Epoch 158/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.8567 - mean_absolute_error: 120.8417\n",
      "Epoch 159/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1867 - mean_absolute_error: 120.7440\n",
      "Epoch 160/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.6314 - mean_absolute_error: 121.1229\n",
      "Epoch 161/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.5765 - mean_absolute_error: 121.0866\n",
      "Epoch 162/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7204 - mean_absolute_error: 120.6257\n",
      "Epoch 163/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1751 - mean_absolute_error: 121.0212\n",
      "Epoch 164/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1842 - mean_absolute_error: 121.5417\n",
      "Epoch 165/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1280 - mean_absolute_error: 120.7544\n",
      "Epoch 166/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4520 - mean_absolute_error: 121.3013\n",
      "Epoch 167/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1655 - mean_absolute_error: 121.1324\n",
      "Epoch 168/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4110 - mean_absolute_error: 120.8287\n",
      "Epoch 169/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.9066 - mean_absolute_error: 120.8349\n",
      "Epoch 170/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9531 - mean_absolute_error: 120.6084\n",
      "Epoch 171/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9569 - mean_absolute_error: 121.1795\n",
      "Epoch 172/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9698 - mean_absolute_error: 120.6610\n",
      "Epoch 173/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1081 - mean_absolute_error: 120.9095\n",
      "Epoch 174/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0928 - mean_absolute_error: 120.5655\n",
      "Epoch 175/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0229 - mean_absolute_error: 120.9377\n",
      "Epoch 176/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.2295 - mean_absolute_error: 120.2728\n",
      "Epoch 177/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1148 - mean_absolute_error: 120.9873\n",
      "Epoch 178/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0343 - mean_absolute_error: 120.5110\n",
      "Epoch 179/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3129 - mean_absolute_error: 120.1613\n",
      "Epoch 180/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7923 - mean_absolute_error: 120.1636\n",
      "Epoch 181/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8788 - mean_absolute_error: 120.6963\n",
      "Epoch 182/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9553 - mean_absolute_error: 120.1233\n",
      "Epoch 183/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8549 - mean_absolute_error: 120.4416\n",
      "Epoch 184/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2222 - mean_absolute_error: 120.6623\n",
      "Epoch 185/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.7197 - mean_absolute_error: 120.1907\n",
      "Epoch 186/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3435 - mean_absolute_error: 120.5932\n",
      "Epoch 187/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1593 - mean_absolute_error: 120.6154\n",
      "Epoch 188/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7320 - mean_absolute_error: 120.2907\n",
      "Epoch 189/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8856 - mean_absolute_error: 120.0192\n",
      "Epoch 190/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7927 - mean_absolute_error: 120.4590\n",
      "Epoch 191/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0453 - mean_absolute_error: 120.3930\n",
      "Epoch 192/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9510 - mean_absolute_error: 120.5120\n",
      "Epoch 193/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2568 - mean_absolute_error: 120.8004\n",
      "Epoch 194/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.6443 - mean_absolute_error: 120.8340\n",
      "Epoch 195/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9680 - mean_absolute_error: 120.9277\n",
      "Epoch 196/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1766 - mean_absolute_error: 121.1347\n",
      "Epoch 197/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2815 - mean_absolute_error: 121.1648\n",
      "Epoch 198/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4414 - mean_absolute_error: 120.4695\n",
      "Epoch 199/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1212 - mean_absolute_error: 120.5734\n",
      "Epoch 200/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4531 - mean_absolute_error: 121.8967\n",
      "Epoch 201/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0419 - mean_absolute_error: 120.8192\n",
      "Epoch 202/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2084 - mean_absolute_error: 121.8085\n",
      "Epoch 203/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4548 - mean_absolute_error: 121.1459\n",
      "Epoch 204/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0742 - mean_absolute_error: 121.3543\n",
      "Epoch 205/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3982 - mean_absolute_error: 120.5686\n",
      "Epoch 206/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.4808 - mean_absolute_error: 120.9566\n",
      "Epoch 207/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 58.5355 - mean_absolute_error: 120.8531\n",
      "Epoch 208/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4657 - mean_absolute_error: 121.1705\n",
      "Epoch 209/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9635 - mean_absolute_error: 120.9321\n",
      "Epoch 210/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.6280 - mean_absolute_error: 120.2423\n",
      "Epoch 211/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9114 - mean_absolute_error: 120.9469\n",
      "Epoch 212/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1450 - mean_absolute_error: 120.7363\n",
      "Epoch 213/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.3762 - mean_absolute_error: 120.6264\n",
      "Epoch 214/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0452 - mean_absolute_error: 120.8239\n",
      "Epoch 215/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2719 - mean_absolute_error: 121.0229\n",
      "Epoch 216/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7910 - mean_absolute_error: 120.8393\n",
      "Epoch 217/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.6730 - mean_absolute_error: 119.9929\n",
      "Epoch 218/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.5283 - mean_absolute_error: 121.1121\n",
      "Epoch 219/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8501 - mean_absolute_error: 120.3219\n",
      "Epoch 220/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3392 - mean_absolute_error: 120.9628\n",
      "Epoch 221/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2952 - mean_absolute_error: 120.2025\n",
      "Epoch 222/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.5187 - mean_absolute_error: 120.8339\n",
      "Epoch 223/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.4634 - mean_absolute_error: 120.0370\n",
      "Epoch 224/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2180 - mean_absolute_error: 120.7648\n",
      "Epoch 225/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4208 - mean_absolute_error: 121.1666\n",
      "Epoch 226/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2566 - mean_absolute_error: 121.1706\n",
      "Epoch 227/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.3355 - mean_absolute_error: 120.2582\n",
      "Epoch 228/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8769 - mean_absolute_error: 120.9379\n",
      "Epoch 229/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9034 - mean_absolute_error: 120.6549\n",
      "Epoch 230/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.9175 - mean_absolute_error: 121.2038\n",
      "Epoch 231/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9706 - mean_absolute_error: 120.6016\n",
      "Epoch 232/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1724 - mean_absolute_error: 121.0908\n",
      "Epoch 233/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8157 - mean_absolute_error: 121.2310\n",
      "Epoch 234/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9935 - mean_absolute_error: 120.3174\n",
      "Epoch 235/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2170 - mean_absolute_error: 120.7917\n",
      "Epoch 236/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9617 - mean_absolute_error: 120.3848\n",
      "Epoch 237/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.2297 - mean_absolute_error: 120.4943\n",
      "Epoch 238/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.0596 - mean_absolute_error: 121.1755\n",
      "Epoch 239/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.1384 - mean_absolute_error: 121.0229\n",
      "Epoch 240/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7630 - mean_absolute_error: 120.1498\n",
      "Epoch 241/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.9968 - mean_absolute_error: 120.8896\n",
      "Epoch 242/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.2232 - mean_absolute_error: 120.9527\n",
      "Epoch 243/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4212 - mean_absolute_error: 121.3329\n",
      "Epoch 244/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3598 - mean_absolute_error: 121.0752\n",
      "Epoch 245/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.6275 - mean_absolute_error: 121.0285\n",
      "Epoch 246/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.5483 - mean_absolute_error: 119.9582\n",
      "Epoch 247/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.8161 - mean_absolute_error: 121.1170\n",
      "Epoch 248/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4003 - mean_absolute_error: 121.0564\n",
      "Epoch 249/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.2367 - mean_absolute_error: 121.1901\n",
      "Epoch 250/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3297 - mean_absolute_error: 120.6943\n",
      "Epoch 251/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.7177 - mean_absolute_error: 120.9027\n",
      "Epoch 252/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.4171 - mean_absolute_error: 120.4636\n",
      "Epoch 253/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3313 - mean_absolute_error: 121.3633\n",
      "Epoch 254/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.3533 - mean_absolute_error: 120.5366\n",
      "Epoch 255/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 59.9882 - mean_absolute_error: 121.0740\n",
      "Epoch 256/256\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 58.3857 - mean_absolute_error: 119.9573\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 57.4375 - mean_absolute_error: 120.7758\n",
      "Loss: 57.646240234375, Mean Absolute Error: 120.04340362548828, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "fit_model = model.fit(X_train, y_train, epochs=256)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, mae,model_accuracy = model.evaluate(X_test,y_test,verbose=1)\n",
    "print(f\"Loss: {model_loss}, Mean Absolute Error: {mae}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "The r2 score for the initial model:-1.0246613025665283\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "r2 = keras.metrics.R2Score()\n",
    "r2.update_state(y_test,y_pred)\n",
    "result = r2.result()\n",
    "print(f\"The r2 score for the initial model:{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(test):\n",
    "    nn_model = keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = test.Choice('activation',['relu','tanh','sigmoid'])\n",
    "\n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(keras.layers.Dense(units=test.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=20,\n",
    "        step=2), activation=activation, input_dim=len(X.columns)))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(test.Int('num_layers', 1, 10)):\n",
    "        nn_model.add(keras.layers.Dense(units=test.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=20,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "\n",
    "    nn_model.add(keras.layers.Dense(units=1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"mean_absolute_percentage_error\", optimizer='adam', metrics=[keras.metrics.MeanAbsoluteError(),\"accuracy\"])\n",
    "\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kille\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 52s]\n",
      "val_loss: 0.5779699683189392\n",
      "\n",
      "Best val_loss So Far: 0.5748056769371033\n",
      "Total elapsed time: 00h 33m 59s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train,y_train,epochs=20,validation_data=(X_test,y_test),batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'first_units': 19,\n",
       " 'num_layers': 4,\n",
       " 'units_0': 19,\n",
       " 'units_1': 11,\n",
       " 'units_2': 11,\n",
       " 'units_3': 17,\n",
       " 'units_4': 7,\n",
       " 'units_5': 7,\n",
       " 'units_6': 3,\n",
       " 'units_7': 13,\n",
       " 'units_8': 9,\n",
       " 'units_9': 5,\n",
       " 'tuner/epochs': 20,\n",
       " 'tuner/initial_epoch': 7,\n",
       " 'tuner/bracket': 1,\n",
       " 'tuner/round': 1,\n",
       " 'tuner/trial_id': '0023'}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kille\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\kille\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 26 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 - 1s - 2ms/step - accuracy: 0.0000e+00 - loss: 0.5748 - mean_absolute_error: 1.2806\n",
      "Loss: 0.5748056173324585, Mean Absolute Error: 1.2805835008621216, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(1)[0]\n",
    "model_loss, mae,model_accuracy = best_model.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Mean Absolute Error: {mae}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "The r2 score for the optimized model:0.9994945526123047\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "r2 = keras.metrics.R2Score()\n",
    "r2.update_state(y_test,y_pred)\n",
    "result = r2.result()\n",
    "print(f\"The r2 score for the optimized model:{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11482    335.016663\n",
       "44248    109.339996\n",
       "38600    338.619995\n",
       "49318    139.139999\n",
       "42781    119.505997\n",
       "            ...    \n",
       "14446    333.036682\n",
       "18438    276.366669\n",
       "1226     299.679993\n",
       "2213     297.096680\n",
       "15093    267.296661\n",
       "Name: Close, Length: 12736, dtype: float64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[336.00726],\n",
       "       [110.85835],\n",
       "       [340.6901 ],\n",
       "       ...,\n",
       "       [297.2532 ],\n",
       "       [298.45624],\n",
       "       [263.53476]], dtype=float32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
