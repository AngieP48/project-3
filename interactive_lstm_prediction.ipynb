{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "import gradio as gr\n",
    "from datetime import timedelta\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#functions \n",
    "def analyze_tweets(tweets):\n",
    "    #delimiter \n",
    "    tweet_list = tweets.split('|||')\n",
    "       \n",
    "    compound_scores = [sid.polarity_scores(tweet.strip())['compound'] for tweet in tweet_list if tweet.strip()]\n",
    "       \n",
    "    if compound_scores:\n",
    "        average_sentiment = sum(compound_scores) / len(compound_scores)\n",
    "    else:\n",
    "        return \"No  tweets entered.\"\n",
    "    \n",
    "    return f\"The average compound sentiment score for the entered tweets is: {average_sentiment}\"\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        past_60_days = data[i-seq_length:i, 0]\n",
    "        current_features = data[i, [1, 2, 3]]\n",
    "        sequence = np.concatenate((past_60_days, current_features), axis=0)\n",
    "        sequences.append(sequence)\n",
    "        labels.append(data[i, 0])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "def fetch_recent_stock_data(stock_symbol, period='3mo'):\n",
    "    stock_data = yf.download(stock_symbol, period=period)\n",
    "    if stock_data.empty:\n",
    "        raise ValueError(f\"No data found for {stock_symbol}.\")\n",
    "    return stock_data\n",
    "\n",
    "#close prediction and caculate dates\n",
    "def predict_next_day_close_yfinance(stock_symbol, sentiment_score, number_of_tweets):\n",
    "    stock_data = fetch_recent_stock_data(stock_symbol)\n",
    "    if len(stock_data) < 60:\n",
    "        raise ValueError(f\"Not enough data available for {stock_symbol}.\")\n",
    "    \n",
    "    past_60_days_close = stock_data['Close'][-60:].values\n",
    "    most_recent_data = stock_data.iloc[-1]\n",
    "    open_price = most_recent_data['Open']\n",
    "    \n",
    "    past_60_days_scaled = scaler.transform(np.column_stack((past_60_days_close, np.zeros((60, 3)))))[0:60, 0]\n",
    "    current_features_scaled = scaler.transform([[0, sentiment_score, number_of_tweets, open_price]])[0, 1:]\n",
    "    \n",
    "    input_sequence = np.concatenate((past_60_days_scaled, current_features_scaled), axis=0)\n",
    "    input_sequence = np.reshape(input_sequence, (1, 1, len(input_sequence)))\n",
    "    \n",
    "    predicted_price_scaled = model.predict(input_sequence)\n",
    "    predicted_price = scaler.inverse_transform([[predicted_price_scaled[0][0], 0, 0, 0]])[0][0]\n",
    "\n",
    "    #calculate the next trading day\n",
    "    latest_date = stock_data.index[-1]\n",
    "    next_prediction_date = latest_date + timedelta(days=1)\n",
    "    \n",
    "    return predicted_price, next_prediction_date\n",
    "\n",
    "#train model on selected stock\n",
    "def train_model(stock_symbol):\n",
    "    stock_specific_data = merged_data[merged_data['Stock Name'] == stock_symbol].sort_values(by='Date')\n",
    "    \n",
    "    training_columns = ['Close', 'Average_Sentiment', 'Number_of_Tweets', 'Open']\n",
    "    training_data = stock_specific_data[training_columns].dropna()\n",
    "    \n",
    "    #scaling\n",
    "    global scaler  # Make scaler global to use it in other functions\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_training_data = scaler.fit_transform(training_data)\n",
    "    \n",
    "    #sequences\n",
    "    sequence_length = 60\n",
    "    X, y = create_sequences(scaled_training_data, sequence_length)\n",
    "    X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    #LSTM Model\n",
    "    global model  #model global to use it in other functions\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=25))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(x_train, y_train, batch_size=1, epochs=10, validation_data=(x_test, y_test))\n",
    "    \n",
    "    #evaluate the model\n",
    "    evaluation_results = evaluate_model(model, x_test, y_test, scaler)\n",
    "    \n",
    "    return f\"Model trained on {stock_symbol}.\\nEvaluation results:\\nMSE: {evaluation_results['MSE']}\\nMAE: {evaluation_results['MAE']}\\nR²: {evaluation_results['R2']}\"\n",
    "\n",
    "def evaluate_model(model, x_test, y_test, scaler):\n",
    "    #predict on the test data\n",
    "    y_pred_scaled = model.predict(x_test)\n",
    "    \n",
    "    #reshape y_test to match y_pred shape\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "    \n",
    "    #inverse scale\n",
    "    y_pred = scaler.inverse_transform(\n",
    "        np.concatenate([y_pred_scaled, np.zeros((y_pred_scaled.shape[0], 3))], axis=1)\n",
    "    )[:, 0]\n",
    "    y_test_original = scaler.inverse_transform(\n",
    "        np.concatenate([y_test, np.zeros((y_test.shape[0], 3))], axis=1)\n",
    "    )[:, 0]\n",
    "    \n",
    "    #eval metrics\n",
    "    mse = mean_squared_error(y_test_original, y_pred)\n",
    "    mae = mean_absolute_error(y_test_original, y_pred)\n",
    "    r2 = r2_score(y_test_original, y_pred)\n",
    "    \n",
    "    return {'MSE': mse, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "def predict_stock_price(stock_symbol, sentiment_score, number_of_tweets):\n",
    "    \n",
    "    train_message = train_model(stock_symbol)\n",
    "    \n",
    "    \n",
    "    predicted_price, prediction_date = predict_next_day_close_yfinance(stock_symbol, sentiment_score, number_of_tweets)\n",
    "    \n",
    "    return f\"{train_message}\\nThe predicted closing price for {stock_symbol} on {prediction_date.date()} is: {predicted_price}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAPL', 'AMD', 'AMZN', 'DIS', 'GOOG', 'META', 'MSFT', 'NFLX',\n",
       "       'NIO', 'PG', 'TSLA', 'TSM', 'XPEV', 'CRM', 'ZS', 'ENPH', 'PYPL',\n",
       "       'BA', 'COST', 'KO', 'BX', 'F', 'INTC', 'NOC', 'VZ'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data['Stock Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file \n",
    "file_path = 'merged_sentiment_stock_data.csv'\n",
    "merged_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "Running on public URL: https://07e57ed5f280ee256c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://07e57ed5f280ee256c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jdhof\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0495 - val_loss: 0.0216\n",
      "Epoch 2/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0224 - val_loss: 0.0088\n",
      "Epoch 3/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0132 - val_loss: 0.0095\n",
      "Epoch 4/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0111 - val_loss: 0.0092\n",
      "Epoch 5/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0094 - val_loss: 0.0097\n",
      "Epoch 6/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0143 - val_loss: 0.0157\n",
      "Epoch 7/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0119 - val_loss: 0.0079\n",
      "Epoch 8/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0105 - val_loss: 0.0130\n",
      "Epoch 9/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0120 - val_loss: 0.0080\n",
      "Epoch 10/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0119 - val_loss: 0.0061\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jdhof\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\jdhof\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jdhof\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0408 - val_loss: 0.0160\n",
      "Epoch 2/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0160 - val_loss: 0.0115\n",
      "Epoch 3/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0132 - val_loss: 0.0086\n",
      "Epoch 4/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 5/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0076 - val_loss: 0.0202\n",
      "Epoch 6/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0112 - val_loss: 0.0060\n",
      "Epoch 7/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 8/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0108 - val_loss: 0.0089\n",
      "Epoch 9/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0072 - val_loss: 0.0056\n",
      "Epoch 10/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0050 - val_loss: 0.0152\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jdhof\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\jdhof\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\jdhof\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0455 - val_loss: 0.0097\n",
      "Epoch 2/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0151 - val_loss: 0.0101\n",
      "Epoch 3/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0115 - val_loss: 0.0086\n",
      "Epoch 4/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0076 - val_loss: 0.0081\n",
      "Epoch 5/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0074 - val_loss: 0.0056\n",
      "Epoch 6/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0106 - val_loss: 0.0059\n",
      "Epoch 7/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0081 - val_loss: 0.0053\n",
      "Epoch 8/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0074 - val_loss: 0.0071\n",
      "Epoch 9/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0072 - val_loss: 0.0122\n",
      "Epoch 10/10\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0094 - val_loss: 0.0053\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jdhof\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\jdhof\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#stock tab\n",
    "stock_price_tab = gr.Interface(\n",
    "    fn=predict_stock_price,\n",
    "    inputs=[\n",
    "        gr.Dropdown(label=\"Stock Symbol\", choices=['AAPL', 'AMD', 'AMZN', 'DIS', 'GOOG', 'META', 'MSFT', 'NFLX',\n",
    "       'NIO', 'PG', 'TSLA', 'TSM', 'XPEV', 'CRM', 'ZS', 'ENPH', 'PYPL',\n",
    "       'BA', 'COST', 'KO', 'BX', 'F', 'INTC', 'NOC', 'VZ'], value=\"AAPL\"),\n",
    "        gr.Slider(minimum=-1.0, maximum=1.0, value=0.4, label=\"Sentiment Score\"),\n",
    "        gr.Slider(minimum=0, maximum=350, value=50, label=\"Number of Tweets\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Stock Price Prediction\",\n",
    "    description=\"Train the model on the selected stock symbol and predict the next day's closing price.\"\n",
    ")\n",
    "#sentiment tab\n",
    "sentiment_tab = gr.Interface(\n",
    "    fn=analyze_tweets,\n",
    "    inputs=gr.Textbox(\n",
    "        lines=5,\n",
    "        placeholder=\"Enter multiple tweets separated by '|||'\",\n",
    "        label=\"Tweets\"\n",
    "    ),\n",
    "    outputs=\"text\",\n",
    "    title=\"Tweet Sentiment Analysis\",\n",
    "    description=\"Enter multiple tweets separated by '|||', and get the average compound sentiment score.\"\n",
    ")\n",
    "\n",
    "#tabbed interface \n",
    "app = gr.TabbedInterface([stock_price_tab, sentiment_tab], tab_names=[\"Stock Price Prediction\", \"Tweet Sentiment Analysis\"])\n",
    "\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
