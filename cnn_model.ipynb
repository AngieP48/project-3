{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\kille\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define imports\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock Name_AMZN</th>\n",
       "      <th>Stock Name_META</th>\n",
       "      <th>Stock Name_MSFT</th>\n",
       "      <th>Stock Name_PG</th>\n",
       "      <th>Stock Name_TSLA</th>\n",
       "      <th>scaler__Sentiment Score</th>\n",
       "      <th>scaler__Tweet Count</th>\n",
       "      <th>scaler__Open</th>\n",
       "      <th>scaler__High</th>\n",
       "      <th>scaler__Low</th>\n",
       "      <th>scaler__Adj Close</th>\n",
       "      <th>scaler__Volume</th>\n",
       "      <th>scaler__Open/Close Diff</th>\n",
       "      <th>scaler__Prev Close Diff</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.539873</td>\n",
       "      <td>0.240781</td>\n",
       "      <td>0.583085</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.540461</td>\n",
       "      <td>0.539232</td>\n",
       "      <td>0.274489</td>\n",
       "      <td>0.290826</td>\n",
       "      <td>0.253251</td>\n",
       "      <td>268.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500859</td>\n",
       "      <td>0.240781</td>\n",
       "      <td>0.583085</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.540461</td>\n",
       "      <td>0.539232</td>\n",
       "      <td>0.274489</td>\n",
       "      <td>0.290826</td>\n",
       "      <td>0.253251</td>\n",
       "      <td>268.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650445</td>\n",
       "      <td>0.240781</td>\n",
       "      <td>0.583085</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.540461</td>\n",
       "      <td>0.539232</td>\n",
       "      <td>0.274489</td>\n",
       "      <td>0.290826</td>\n",
       "      <td>0.253251</td>\n",
       "      <td>268.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.118405</td>\n",
       "      <td>0.240781</td>\n",
       "      <td>0.583085</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.540461</td>\n",
       "      <td>0.539232</td>\n",
       "      <td>0.274489</td>\n",
       "      <td>0.290826</td>\n",
       "      <td>0.253251</td>\n",
       "      <td>268.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.058672</td>\n",
       "      <td>0.240781</td>\n",
       "      <td>0.583085</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.540461</td>\n",
       "      <td>0.539232</td>\n",
       "      <td>0.274489</td>\n",
       "      <td>0.290826</td>\n",
       "      <td>0.253251</td>\n",
       "      <td>268.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42360</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.729028</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.204230</td>\n",
       "      <td>0.199449</td>\n",
       "      <td>0.205119</td>\n",
       "      <td>0.201333</td>\n",
       "      <td>0.197219</td>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.407088</td>\n",
       "      <td>164.251999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42361</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.242824</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.204230</td>\n",
       "      <td>0.199449</td>\n",
       "      <td>0.205119</td>\n",
       "      <td>0.201333</td>\n",
       "      <td>0.197219</td>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.407088</td>\n",
       "      <td>164.251999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42362</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.386497</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.204230</td>\n",
       "      <td>0.199449</td>\n",
       "      <td>0.205119</td>\n",
       "      <td>0.201333</td>\n",
       "      <td>0.197219</td>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.407088</td>\n",
       "      <td>164.251999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42363</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.204230</td>\n",
       "      <td>0.199449</td>\n",
       "      <td>0.205119</td>\n",
       "      <td>0.201333</td>\n",
       "      <td>0.197219</td>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.407088</td>\n",
       "      <td>164.251999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42364</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.794421</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.204230</td>\n",
       "      <td>0.199449</td>\n",
       "      <td>0.205119</td>\n",
       "      <td>0.201333</td>\n",
       "      <td>0.197219</td>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.407088</td>\n",
       "      <td>164.251999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42365 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Stock Name_AMZN  Stock Name_META  Stock Name_MSFT  Stock Name_PG  \\\n",
       "0                    0                0                0              0   \n",
       "1                    0                0                0              0   \n",
       "2                    0                0                0              0   \n",
       "3                    0                0                0              0   \n",
       "4                    0                0                0              0   \n",
       "...                ...              ...              ...            ...   \n",
       "42360                1                0                0              0   \n",
       "42361                1                0                0              0   \n",
       "42362                1                0                0              0   \n",
       "42363                1                0                0              0   \n",
       "42364                1                0                0              0   \n",
       "\n",
       "       Stock Name_TSLA  scaler__Sentiment Score  scaler__Tweet Count  \\\n",
       "0                    1                 0.539873             0.240781   \n",
       "1                    1                 0.500859             0.240781   \n",
       "2                    1                 0.650445             0.240781   \n",
       "3                    1                 0.118405             0.240781   \n",
       "4                    1                 0.058672             0.240781   \n",
       "...                ...                      ...                  ...   \n",
       "42360                0                 0.729028             0.008677   \n",
       "42361                0                 0.242824             0.008677   \n",
       "42362                0                 0.386497             0.008677   \n",
       "42363                0                 0.974328             0.008677   \n",
       "42364                0                 0.794421             0.008677   \n",
       "\n",
       "       scaler__Open  scaler__High  scaler__Low  scaler__Adj Close  \\\n",
       "0          0.583085      0.577800     0.540461           0.539232   \n",
       "1          0.583085      0.577800     0.540461           0.539232   \n",
       "2          0.583085      0.577800     0.540461           0.539232   \n",
       "3          0.583085      0.577800     0.540461           0.539232   \n",
       "4          0.583085      0.577800     0.540461           0.539232   \n",
       "...             ...           ...          ...                ...   \n",
       "42360      0.204230      0.199449     0.205119           0.201333   \n",
       "42361      0.204230      0.199449     0.205119           0.201333   \n",
       "42362      0.204230      0.199449     0.205119           0.201333   \n",
       "42363      0.204230      0.199449     0.205119           0.201333   \n",
       "42364      0.204230      0.199449     0.205119           0.201333   \n",
       "\n",
       "       scaler__Volume  scaler__Open/Close Diff  scaler__Prev Close Diff  \\\n",
       "0            0.274489                 0.290826                 0.253251   \n",
       "1            0.274489                 0.290826                 0.253251   \n",
       "2            0.274489                 0.290826                 0.253251   \n",
       "3            0.274489                 0.290826                 0.253251   \n",
       "4            0.274489                 0.290826                 0.253251   \n",
       "...               ...                      ...                      ...   \n",
       "42360        0.197219                 0.030981                 0.407088   \n",
       "42361        0.197219                 0.030981                 0.407088   \n",
       "42362        0.197219                 0.030981                 0.407088   \n",
       "42363        0.197219                 0.030981                 0.407088   \n",
       "42364        0.197219                 0.030981                 0.407088   \n",
       "\n",
       "            Close  \n",
       "0      268.209991  \n",
       "1      268.209991  \n",
       "2      268.209991  \n",
       "3      268.209991  \n",
       "4      268.209991  \n",
       "...           ...  \n",
       "42360  164.251999  \n",
       "42361  164.251999  \n",
       "42362  164.251999  \n",
       "42363  164.251999  \n",
       "42364  164.251999  \n",
       "\n",
       "[42365 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define stock_df from pre-generated csv\n",
    "stock_df = pd.read_csv('Resources/final_stock_data.csv',header='infer')\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y values\n",
    "X = stock_df.drop(columns=['Close'])\n",
    "y = stock_df['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kille\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "input_nodes = len(X_train.columns)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units=18,activation='relu',input_dim=input_nodes))\n",
    "model.add(keras.layers.Dense(units=8, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=4,activation='relu'))\n",
    "model.add(keras.layers.Dense(units=8, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=4,activation='relu'))\n",
    "model.add(keras.layers.Dense(units=8,activation='relu'))\n",
    "model.add(keras.layers.Dense(1,activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_percentage_error', metrics=[keras.metrics.MeanAbsoluteError(),'accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 49.4153 - mean_absolute_error: 137.2945\n",
      "Epoch 2/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.6940 - mean_absolute_error: 1.9598\n",
      "Epoch 3/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.6194 - mean_absolute_error: 1.7367\n",
      "Epoch 4/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.5368 - mean_absolute_error: 1.4961\n",
      "Epoch 5/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.4978 - mean_absolute_error: 1.3820\n",
      "Epoch 6/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.4586 - mean_absolute_error: 1.2715\n",
      "Epoch 7/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.4373 - mean_absolute_error: 1.2058\n",
      "Epoch 8/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.4198 - mean_absolute_error: 1.1520\n",
      "Epoch 9/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.4028 - mean_absolute_error: 1.1034\n",
      "Epoch 10/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.3851 - mean_absolute_error: 1.0581\n",
      "Epoch 11/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.3822 - mean_absolute_error: 1.0483\n",
      "Epoch 12/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.3582 - mean_absolute_error: 0.9760\n",
      "Epoch 13/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.3497 - mean_absolute_error: 0.9545\n",
      "Epoch 14/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.3383 - mean_absolute_error: 0.9213\n",
      "Epoch 15/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.3338 - mean_absolute_error: 0.9091\n",
      "Epoch 16/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.3088 - mean_absolute_error: 0.8371\n",
      "Epoch 17/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.3078 - mean_absolute_error: 0.8330\n",
      "Epoch 18/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.2890 - mean_absolute_error: 0.7842\n",
      "Epoch 19/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2804 - mean_absolute_error: 0.7583\n",
      "Epoch 20/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.2728 - mean_absolute_error: 0.7316\n",
      "Epoch 21/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2728 - mean_absolute_error: 0.7343\n",
      "Epoch 22/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2673 - mean_absolute_error: 0.7163\n",
      "Epoch 23/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2492 - mean_absolute_error: 0.6664\n",
      "Epoch 24/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2380 - mean_absolute_error: 0.6341\n",
      "Epoch 25/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.2347 - mean_absolute_error: 0.6215\n",
      "Epoch 26/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2162 - mean_absolute_error: 0.5703\n",
      "Epoch 27/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2092 - mean_absolute_error: 0.5542\n",
      "Epoch 28/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2115 - mean_absolute_error: 0.5557\n",
      "Epoch 29/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2021 - mean_absolute_error: 0.5291\n",
      "Epoch 30/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1918 - mean_absolute_error: 0.4963\n",
      "Epoch 31/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1827 - mean_absolute_error: 0.4657\n",
      "Epoch 32/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1676 - mean_absolute_error: 0.4200\n",
      "Epoch 33/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1635 - mean_absolute_error: 0.4116\n",
      "Epoch 34/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1631 - mean_absolute_error: 0.4076\n",
      "Epoch 35/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1632 - mean_absolute_error: 0.4114\n",
      "Epoch 36/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1501 - mean_absolute_error: 0.3710\n",
      "Epoch 37/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1747 - mean_absolute_error: 0.4420\n",
      "Epoch 38/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1772 - mean_absolute_error: 0.4468\n",
      "Epoch 39/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1558 - mean_absolute_error: 0.3868\n",
      "Epoch 40/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1959 - mean_absolute_error: 0.4994\n",
      "Epoch 41/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1513 - mean_absolute_error: 0.3732\n",
      "Epoch 42/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1553 - mean_absolute_error: 0.3868\n",
      "Epoch 43/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1669 - mean_absolute_error: 0.4188\n",
      "Epoch 44/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1687 - mean_absolute_error: 0.4233\n",
      "Epoch 45/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1694 - mean_absolute_error: 0.4249\n",
      "Epoch 46/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1981 - mean_absolute_error: 0.5037\n",
      "Epoch 47/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1734 - mean_absolute_error: 0.4377\n",
      "Epoch 48/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1702 - mean_absolute_error: 0.4290\n",
      "Epoch 49/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1519 - mean_absolute_error: 0.3735\n",
      "Epoch 50/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1757 - mean_absolute_error: 0.4450\n",
      "Epoch 51/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1701 - mean_absolute_error: 0.4253\n",
      "Epoch 52/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1761 - mean_absolute_error: 0.4416\n",
      "Epoch 53/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1686 - mean_absolute_error: 0.4242\n",
      "Epoch 54/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1671 - mean_absolute_error: 0.4156\n",
      "Epoch 55/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1909 - mean_absolute_error: 0.4824\n",
      "Epoch 56/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.2021 - mean_absolute_error: 0.5153\n",
      "Epoch 57/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1639 - mean_absolute_error: 0.4104\n",
      "Epoch 58/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1709 - mean_absolute_error: 0.4297\n",
      "Epoch 59/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1984 - mean_absolute_error: 0.5087\n",
      "Epoch 60/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1773 - mean_absolute_error: 0.4492\n",
      "Epoch 61/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1859 - mean_absolute_error: 0.4688\n",
      "Epoch 62/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1630 - mean_absolute_error: 0.4080\n",
      "Epoch 63/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1784 - mean_absolute_error: 0.4482\n",
      "Epoch 64/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1572 - mean_absolute_error: 0.3926\n",
      "Epoch 65/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1648 - mean_absolute_error: 0.4127\n",
      "Epoch 66/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2063 - mean_absolute_error: 0.5284\n",
      "Epoch 67/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1757 - mean_absolute_error: 0.4408\n",
      "Epoch 68/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1561 - mean_absolute_error: 0.3874\n",
      "Epoch 69/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1647 - mean_absolute_error: 0.4102\n",
      "Epoch 70/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1716 - mean_absolute_error: 0.4324\n",
      "Epoch 71/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1707 - mean_absolute_error: 0.4290\n",
      "Epoch 72/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2033 - mean_absolute_error: 0.5218\n",
      "Epoch 73/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1522 - mean_absolute_error: 0.3762\n",
      "Epoch 74/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1722 - mean_absolute_error: 0.4326\n",
      "Epoch 75/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1887 - mean_absolute_error: 0.4804\n",
      "Epoch 76/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1824 - mean_absolute_error: 0.4602\n",
      "Epoch 77/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1588 - mean_absolute_error: 0.3971\n",
      "Epoch 78/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1676 - mean_absolute_error: 0.4200\n",
      "Epoch 79/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1730 - mean_absolute_error: 0.4369\n",
      "Epoch 80/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1741 - mean_absolute_error: 0.4400\n",
      "Epoch 81/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1686 - mean_absolute_error: 0.4214\n",
      "Epoch 82/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1672 - mean_absolute_error: 0.4213\n",
      "Epoch 83/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1825 - mean_absolute_error: 0.4638\n",
      "Epoch 84/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1685 - mean_absolute_error: 0.4240\n",
      "Epoch 85/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1643 - mean_absolute_error: 0.4083\n",
      "Epoch 86/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1883 - mean_absolute_error: 0.4733\n",
      "Epoch 87/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1647 - mean_absolute_error: 0.4144\n",
      "Epoch 88/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.2012 - mean_absolute_error: 0.5157\n",
      "Epoch 89/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1753 - mean_absolute_error: 0.4400\n",
      "Epoch 90/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1516 - mean_absolute_error: 0.3774\n",
      "Epoch 91/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1650 - mean_absolute_error: 0.4104\n",
      "Epoch 92/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1756 - mean_absolute_error: 0.4443\n",
      "Epoch 93/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1814 - mean_absolute_error: 0.4598\n",
      "Epoch 94/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1807 - mean_absolute_error: 0.4590\n",
      "Epoch 95/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2052 - mean_absolute_error: 0.5257\n",
      "Epoch 96/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1505 - mean_absolute_error: 0.3743\n",
      "Epoch 97/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1552 - mean_absolute_error: 0.3851\n",
      "Epoch 98/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1683 - mean_absolute_error: 0.4233\n",
      "Epoch 99/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1660 - mean_absolute_error: 0.4188\n",
      "Epoch 100/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1746 - mean_absolute_error: 0.4423\n",
      "Epoch 101/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1521 - mean_absolute_error: 0.3775\n",
      "Epoch 102/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1729 - mean_absolute_error: 0.4347\n",
      "Epoch 103/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1672 - mean_absolute_error: 0.4233\n",
      "Epoch 104/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1812 - mean_absolute_error: 0.4562\n",
      "Epoch 105/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1517 - mean_absolute_error: 0.3763\n",
      "Epoch 106/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1638 - mean_absolute_error: 0.4083\n",
      "Epoch 107/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1729 - mean_absolute_error: 0.4392\n",
      "Epoch 108/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1760 - mean_absolute_error: 0.4451\n",
      "Epoch 109/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1805 - mean_absolute_error: 0.4587\n",
      "Epoch 110/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1922 - mean_absolute_error: 0.4919\n",
      "Epoch 111/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1564 - mean_absolute_error: 0.3894\n",
      "Epoch 112/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1689 - mean_absolute_error: 0.4245\n",
      "Epoch 113/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1538 - mean_absolute_error: 0.3780\n",
      "Epoch 114/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1973 - mean_absolute_error: 0.5063\n",
      "Epoch 115/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1549 - mean_absolute_error: 0.3857\n",
      "Epoch 116/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1688 - mean_absolute_error: 0.4203\n",
      "Epoch 117/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1501 - mean_absolute_error: 0.3725\n",
      "Epoch 118/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1742 - mean_absolute_error: 0.4388\n",
      "Epoch 119/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1828 - mean_absolute_error: 0.4662\n",
      "Epoch 120/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1699 - mean_absolute_error: 0.4252\n",
      "Epoch 121/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1694 - mean_absolute_error: 0.4264\n",
      "Epoch 122/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1624 - mean_absolute_error: 0.4054\n",
      "Epoch 123/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1845 - mean_absolute_error: 0.4717\n",
      "Epoch 124/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1621 - mean_absolute_error: 0.4035\n",
      "Epoch 125/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1677 - mean_absolute_error: 0.4229\n",
      "Epoch 126/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1621 - mean_absolute_error: 0.4029\n",
      "Epoch 127/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1726 - mean_absolute_error: 0.4356\n",
      "Epoch 128/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1540 - mean_absolute_error: 0.3819\n",
      "Epoch 129/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1726 - mean_absolute_error: 0.4363\n",
      "Epoch 130/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1649 - mean_absolute_error: 0.4142\n",
      "Epoch 131/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1834 - mean_absolute_error: 0.4657\n",
      "Epoch 132/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1454 - mean_absolute_error: 0.3591\n",
      "Epoch 133/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1637 - mean_absolute_error: 0.4092\n",
      "Epoch 134/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1582 - mean_absolute_error: 0.3942\n",
      "Epoch 135/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1452 - mean_absolute_error: 0.3556\n",
      "Epoch 136/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1696 - mean_absolute_error: 0.4286\n",
      "Epoch 137/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1591 - mean_absolute_error: 0.3972\n",
      "Epoch 138/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1724 - mean_absolute_error: 0.4345\n",
      "Epoch 139/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1733 - mean_absolute_error: 0.4383\n",
      "Epoch 140/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1583 - mean_absolute_error: 0.3919\n",
      "Epoch 141/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1696 - mean_absolute_error: 0.4283\n",
      "Epoch 142/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1598 - mean_absolute_error: 0.3976\n",
      "Epoch 143/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1698 - mean_absolute_error: 0.4276\n",
      "Epoch 144/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1726 - mean_absolute_error: 0.4381\n",
      "Epoch 145/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1556 - mean_absolute_error: 0.3865\n",
      "Epoch 146/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1562 - mean_absolute_error: 0.3892\n",
      "Epoch 147/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1756 - mean_absolute_error: 0.4416\n",
      "Epoch 148/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1754 - mean_absolute_error: 0.4459\n",
      "Epoch 149/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1596 - mean_absolute_error: 0.3975\n",
      "Epoch 150/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1680 - mean_absolute_error: 0.4203\n",
      "Epoch 151/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1674 - mean_absolute_error: 0.4205\n",
      "Epoch 152/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1600 - mean_absolute_error: 0.4009\n",
      "Epoch 153/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1675 - mean_absolute_error: 0.4189\n",
      "Epoch 154/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1729 - mean_absolute_error: 0.4377\n",
      "Epoch 155/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1785 - mean_absolute_error: 0.4497\n",
      "Epoch 156/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1682 - mean_absolute_error: 0.4240\n",
      "Epoch 157/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1558 - mean_absolute_error: 0.3885\n",
      "Epoch 158/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1596 - mean_absolute_error: 0.4007\n",
      "Epoch 159/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1899 - mean_absolute_error: 0.4859\n",
      "Epoch 160/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1547 - mean_absolute_error: 0.3866\n",
      "Epoch 161/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1467 - mean_absolute_error: 0.3630\n",
      "Epoch 162/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1694 - mean_absolute_error: 0.4300\n",
      "Epoch 163/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1585 - mean_absolute_error: 0.3957\n",
      "Epoch 164/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1624 - mean_absolute_error: 0.4067\n",
      "Epoch 165/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1554 - mean_absolute_error: 0.3888\n",
      "Epoch 166/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1741 - mean_absolute_error: 0.4420\n",
      "Epoch 167/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1758 - mean_absolute_error: 0.4463\n",
      "Epoch 168/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1771 - mean_absolute_error: 0.4508\n",
      "Epoch 169/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1495 - mean_absolute_error: 0.3669\n",
      "Epoch 170/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1711 - mean_absolute_error: 0.4304\n",
      "Epoch 171/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1705 - mean_absolute_error: 0.4308\n",
      "Epoch 172/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.1634 - mean_absolute_error: 0.4073\n",
      "Epoch 173/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1639 - mean_absolute_error: 0.4122\n",
      "Epoch 174/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1672 - mean_absolute_error: 0.4180\n",
      "Epoch 175/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1880 - mean_absolute_error: 0.4805\n",
      "Epoch 176/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1638 - mean_absolute_error: 0.4096\n",
      "Epoch 177/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1739 - mean_absolute_error: 0.4393\n",
      "Epoch 178/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1516 - mean_absolute_error: 0.3760\n",
      "Epoch 179/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1741 - mean_absolute_error: 0.4368\n",
      "Epoch 180/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1648 - mean_absolute_error: 0.4160\n",
      "Epoch 181/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1732 - mean_absolute_error: 0.4378\n",
      "Epoch 182/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1656 - mean_absolute_error: 0.4172\n",
      "Epoch 183/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1640 - mean_absolute_error: 0.4126\n",
      "Epoch 184/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1665 - mean_absolute_error: 0.4187\n",
      "Epoch 185/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1666 - mean_absolute_error: 0.4211\n",
      "Epoch 186/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1864 - mean_absolute_error: 0.4756\n",
      "Epoch 187/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1681 - mean_absolute_error: 0.4235\n",
      "Epoch 188/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1601 - mean_absolute_error: 0.4012\n",
      "Epoch 189/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1756 - mean_absolute_error: 0.4493\n",
      "Epoch 190/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1761 - mean_absolute_error: 0.4442\n",
      "Epoch 191/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1555 - mean_absolute_error: 0.3887\n",
      "Epoch 192/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1667 - mean_absolute_error: 0.4182\n",
      "Epoch 193/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1803 - mean_absolute_error: 0.4563\n",
      "Epoch 194/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1485 - mean_absolute_error: 0.3672\n",
      "Epoch 195/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1658 - mean_absolute_error: 0.4145\n",
      "Epoch 196/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1658 - mean_absolute_error: 0.4134\n",
      "Epoch 197/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1670 - mean_absolute_error: 0.4217\n",
      "Epoch 198/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1485 - mean_absolute_error: 0.3657\n",
      "Epoch 199/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.1688 - mean_absolute_error: 0.4251\n",
      "Epoch 200/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1562 - mean_absolute_error: 0.3895\n",
      "Epoch 201/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1634 - mean_absolute_error: 0.4112\n",
      "Epoch 202/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1528 - mean_absolute_error: 0.3802\n",
      "Epoch 203/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1536 - mean_absolute_error: 0.3829\n",
      "Epoch 204/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1708 - mean_absolute_error: 0.4322\n",
      "Epoch 205/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1748 - mean_absolute_error: 0.4410\n",
      "Epoch 206/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1572 - mean_absolute_error: 0.3935\n",
      "Epoch 207/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1594 - mean_absolute_error: 0.4015\n",
      "Epoch 208/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1602 - mean_absolute_error: 0.4004\n",
      "Epoch 209/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1618 - mean_absolute_error: 0.4090\n",
      "Epoch 210/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1588 - mean_absolute_error: 0.3978\n",
      "Epoch 211/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1696 - mean_absolute_error: 0.4296\n",
      "Epoch 212/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1755 - mean_absolute_error: 0.4444\n",
      "Epoch 213/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1389 - mean_absolute_error: 0.3398\n",
      "Epoch 214/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1657 - mean_absolute_error: 0.4171\n",
      "Epoch 215/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1730 - mean_absolute_error: 0.4383\n",
      "Epoch 216/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1576 - mean_absolute_error: 0.3956\n",
      "Epoch 217/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1545 - mean_absolute_error: 0.3822\n",
      "Epoch 218/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1804 - mean_absolute_error: 0.4594\n",
      "Epoch 219/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1544 - mean_absolute_error: 0.3842\n",
      "Epoch 220/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1585 - mean_absolute_error: 0.3981\n",
      "Epoch 221/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1449 - mean_absolute_error: 0.3580\n",
      "Epoch 222/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1647 - mean_absolute_error: 0.4130\n",
      "Epoch 223/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1640 - mean_absolute_error: 0.4103\n",
      "Epoch 224/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1762 - mean_absolute_error: 0.4464\n",
      "Epoch 225/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1552 - mean_absolute_error: 0.3868\n",
      "Epoch 226/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1649 - mean_absolute_error: 0.4126\n",
      "Epoch 227/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1615 - mean_absolute_error: 0.4047\n",
      "Epoch 228/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1646 - mean_absolute_error: 0.4110\n",
      "Epoch 229/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1588 - mean_absolute_error: 0.3971\n",
      "Epoch 230/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1512 - mean_absolute_error: 0.3770\n",
      "Epoch 231/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1610 - mean_absolute_error: 0.4037\n",
      "Epoch 232/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1419 - mean_absolute_error: 0.3497\n",
      "Epoch 233/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1708 - mean_absolute_error: 0.4315\n",
      "Epoch 234/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1471 - mean_absolute_error: 0.3644\n",
      "Epoch 235/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1511 - mean_absolute_error: 0.3757\n",
      "Epoch 236/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1493 - mean_absolute_error: 0.3681\n",
      "Epoch 237/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1488 - mean_absolute_error: 0.3687\n",
      "Epoch 238/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1800 - mean_absolute_error: 0.4582\n",
      "Epoch 239/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1589 - mean_absolute_error: 0.3982\n",
      "Epoch 240/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1442 - mean_absolute_error: 0.3602\n",
      "Epoch 241/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1667 - mean_absolute_error: 0.4171\n",
      "Epoch 242/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1572 - mean_absolute_error: 0.3945\n",
      "Epoch 243/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1779 - mean_absolute_error: 0.4520\n",
      "Epoch 244/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1571 - mean_absolute_error: 0.3907\n",
      "Epoch 245/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1454 - mean_absolute_error: 0.3578\n",
      "Epoch 246/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.1403 - mean_absolute_error: 0.3464\n",
      "Epoch 247/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1684 - mean_absolute_error: 0.4233\n",
      "Epoch 248/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1576 - mean_absolute_error: 0.3950\n",
      "Epoch 249/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1426 - mean_absolute_error: 0.3508\n",
      "Epoch 250/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1889 - mean_absolute_error: 0.4888\n",
      "Epoch 251/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1575 - mean_absolute_error: 0.3902\n",
      "Epoch 252/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1439 - mean_absolute_error: 0.3552\n",
      "Epoch 253/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1856 - mean_absolute_error: 0.4721\n",
      "Epoch 254/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1785 - mean_absolute_error: 0.4532\n",
      "Epoch 255/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1692 - mean_absolute_error: 0.4266\n",
      "Epoch 256/256\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1691 - mean_absolute_error: 0.4295\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.1531 - mean_absolute_error: 0.3840\n",
      "Loss: 0.15473857522010803, Mean Absolute Error: 0.3861406147480011, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Fit and evaluate model\n",
    "fit_model = model.fit(X_train, y_train, epochs=256)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, mae,model_accuracy = model.evaluate(X_test,y_test,verbose=1)\n",
    "print(f\"Loss: {model_loss}, Mean Absolute Error: {mae}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "The r2 score for the initial model:0.9999610781669617\n"
     ]
    }
   ],
   "source": [
    "# R2 score for initial model comparison of y_pred and y_test values from 0 to 1\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = keras.metrics.R2Score()\n",
    "r2.update_state(y_test,y_pred)\n",
    "result = r2.result()\n",
    "print(f\"The r2 score for the initial model:{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(test):\n",
    "    nn_model = keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = test.Choice('activation',['relu','tanh','sigmoid'])\n",
    "\n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(keras.layers.Dense(units=test.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=20,\n",
    "        step=2), activation=activation, input_dim=len(X.columns)))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(test.Int('num_layers', 1, 10)):\n",
    "        nn_model.add(keras.layers.Dense(units=test.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=20,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "\n",
    "    nn_model.add(keras.layers.Dense(units=1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"mean_absolute_percentage_error\", optimizer='adam', metrics=[keras.metrics.MeanAbsoluteError(),\"accuracy\"])\n",
    "\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kille\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the keras_tuner function for optimization\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 30s]\n",
      "val_loss: 81.70697784423828\n",
      "\n",
      "Best val_loss So Far: 0.48244255781173706\n",
      "Total elapsed time: 01h 44m 01s\n"
     ]
    }
   ],
   "source": [
    "# Run the search function to find the best hyperparameters and model.\n",
    "tuner.search(X_train,y_train,epochs=100,validation_data=(X_test,y_test),batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'first_units': 9,\n",
       " 'num_layers': 8,\n",
       " 'units_0': 11,\n",
       " 'units_1': 19,\n",
       " 'units_2': 17,\n",
       " 'units_3': 19,\n",
       " 'units_4': 11,\n",
       " 'units_5': 13,\n",
       " 'units_6': 15,\n",
       " 'units_7': 5,\n",
       " 'units_8': 11,\n",
       " 'units_9': 9,\n",
       " 'tuner/epochs': 20,\n",
       " 'tuner/initial_epoch': 0,\n",
       " 'tuner/bracket': 0,\n",
       " 'tuner/round': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List best hyperparameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kille\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\kille\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 - 1s - 6ms/step - accuracy: 0.0000e+00 - loss: 0.4824 - mean_absolute_error: 1.3271\n",
      "Loss: 0.48244255781173706, Mean Absolute Error: 1.3271336555480957, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from the tuner\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "model_loss, mae,model_accuracy = best_model.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Mean Absolute Error: {mae}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "The r2 score for the optimized model:0.9990188479423523\n"
     ]
    }
   ],
   "source": [
    "# R2 score compares y_pred to y_test values from 0 to 1\n",
    "y_pred = best_model.predict(X_test)\n",
    "r2 = keras.metrics.R2Score()\n",
    "r2.update_state(y_test,y_pred)\n",
    "result = r2.result()\n",
    "print(f\"The r2 score for the optimized model:{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30583    264.839996\n",
       "33104    336.989990\n",
       "12528    340.790009\n",
       "27422    339.476654\n",
       "14773    301.796661\n",
       "            ...    \n",
       "32218    305.940002\n",
       "3585     290.423340\n",
       "33459    138.179993\n",
       "9060     236.473328\n",
       "10017    262.369995\n",
       "Name: Close, Length: 8473, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_test for comparison\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.9755"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred comparison value\n",
    "y_pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for later use.\n",
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the closing price with given data\n",
    "def predict(date,stock_name,tweet,open,high,low,adj_close,volume):\n",
    "    model = keras.models.load_model('model.keras')\n",
    "    df = pd.DataFrame(data={'Date':[date],\n",
    "                            'Stock Name':[stock_name],\n",
    "                            'Tweet':[tweet],\n",
    "                            'Open':[open],\n",
    "                            'High':[high],\n",
    "                            'Low':[low],\n",
    "                            'Adj Close':[adj_close],\n",
    "                            'Volume':[volume]})\n",
    "    \n",
    "    df['Sentiment Score'] = df['Tweet'].apply(util.get_score)\n",
    "    \n",
    "    df = util.add_features(df)\n",
    "    \n",
    "    df = df.drop(columns=['Date','Datetime','Tweet'])\n",
    "    \n",
    "    numerical_df = util.numerical_pre_processing(df)\n",
    "    \n",
    "    numerical_df = numerical_df.reset_index(drop=True)\n",
    "    categorical_df = util.categorical_pre_processing(df)\n",
    "    categorical_df = categorical_df.reset_index(drop=True)\n",
    "    stock_df = pd.concat([categorical_df,numerical_df],axis=1)\n",
    "\n",
    "    pred = model.predict(stock_df)\n",
    "    print(stock_df)\n",
    "    return pred[0][0]\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Open    High     Low  Adj Close  Volume  Sentiment Score  Tweet Count  \\\n",
      "0  210.01  288.57  198.21     277.77  210000          -0.3612            1   \n",
      "\n",
      "   Open/Close Diff  Prev Close Diff  \n",
      "0            67.76       281.361553  \n",
      "       Open      High       Low  Adj Close  Volume  Sentiment Score  \\\n",
      "0  0.999977  0.999988  0.999975   0.999987     1.0         0.115409   \n",
      "\n",
      "   Tweet Count  Open/Close Diff  Prev Close Diff  \n",
      "0          0.5         0.999782         0.999987  \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "   Stock Name_TSLA  Stock Name_MSFT  Stock Name_PG  Stock Name_META  \\\n",
      "0                0                1              0                0   \n",
      "\n",
      "   Stock Name_AMZN      Open      High       Low  Adj Close  Volume  \\\n",
      "0                0  0.999977  0.999988  0.999975   0.999987     1.0   \n",
      "\n",
      "   Sentiment Score  Tweet Count  Open/Close Diff  Prev Close Diff  \n",
      "0         0.115409          0.5         0.999782         0.999987  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "162.92062"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code for debugging\n",
    "pred = predict('2022-01-18','MSFT','Microsoft sucks',210.01,288.57,198.21,277.77,210000)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Open    High     Low  Adj Close  Volume  Sentiment Score  Tweet Count  \\\n",
      "0  110.23  150.21  130.45     145.28    1000          -0.5096            1   \n",
      "\n",
      "   Open/Close Diff  Prev Close Diff  \n",
      "0            35.05       212.347811  \n",
      "       Open      High       Low  Adj Close    Volume  Sentiment Score  \\\n",
      "0  0.999918  0.999956  0.999941   0.999953  0.999999         0.206155   \n",
      "\n",
      "   Tweet Count  Open/Close Diff  Prev Close Diff  \n",
      "0          0.5         0.999187         0.999978  \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "   Stock Name_TSLA  Stock Name_MSFT  Stock Name_PG  Stock Name_META  \\\n",
      "0                1                0              0                0   \n",
      "\n",
      "   Stock Name_AMZN      Open      High       Low  Adj Close    Volume  \\\n",
      "0                0  0.999918  0.999956  0.999941   0.999953  0.999999   \n",
      "\n",
      "   Sentiment Score  Tweet Count  Open/Close Diff  Prev Close Diff  \n",
      "0         0.206155          0.5         0.999187         0.999978  \n",
      "     Open    High     Low  Adj Close  Volume  Sentiment Score  Tweet Count  \\\n",
      "0  110.23  150.21  130.45     145.28   10000          -0.5096            1   \n",
      "\n",
      "   Open/Close Diff  Prev Close Diff  \n",
      "0            35.05       317.045784  \n",
      "       Open      High       Low  Adj Close  Volume  Sentiment Score  \\\n",
      "0  0.999918  0.999956  0.999941   0.999953     1.0         0.206155   \n",
      "\n",
      "   Tweet Count  Open/Close Diff  Prev Close Diff  \n",
      "0          0.5         0.999187          0.99999  \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "   Stock Name_TSLA  Stock Name_MSFT  Stock Name_PG  Stock Name_META  \\\n",
      "0                1                0              0                0   \n",
      "\n",
      "   Stock Name_AMZN      Open      High       Low  Adj Close  Volume  \\\n",
      "0                0  0.999918  0.999956  0.999941   0.999953     1.0   \n",
      "\n",
      "   Sentiment Score  Tweet Count  Open/Close Diff  Prev Close Diff  \n",
      "0         0.206155          0.5         0.999187          0.99999  \n"
     ]
    }
   ],
   "source": [
    "# Define application\n",
    "app = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs = [\n",
    "        gr.Textbox(lines=1,label=\"Please enter a Date in YYYY-MM-DD:\"),\n",
    "        gr.Dropdown(choices=['TSLA', 'MSFT', 'PG', 'META', 'AMZN'],\n",
    "                    label='Select a stock'),\n",
    "        gr.Textbox(lines=10,label=\"Enter a Tweet about the company:\"),\n",
    "        gr.Number(label='Enter the open price as float:'),\n",
    "        gr.Number(label='Enter the high price as float:'),\n",
    "        gr.Number(label='Enter the low price as float:'),\n",
    "        gr.Number(label='Enter the adj close price as float:'),\n",
    "        gr.Number(label='Enter the volume as an integer:')\n",
    "        ],\n",
    "    outputs=gr.Number( label=\"Predicted Close price:\"))\n",
    "    \n",
    "# Launch the app.\n",
    "app.launch(show_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
